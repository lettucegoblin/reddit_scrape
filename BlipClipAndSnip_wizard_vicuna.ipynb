{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/transformers.git\n",
    "%pip install Pillow redvid\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from redvid import Downloader\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def isMediaDomain(url):\n",
    "  for mediaDomain in mediaDomains.keys():\n",
    "    if mediaDomain in url or url.startswith('self.'):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "video_input_file = 'video.mp4'\n",
    "\n",
    "\n",
    "img_output_file = 'image.jpg'\n",
    "video_output_file = img_output_file#'video.jpg'\n",
    "\n",
    "def downloadRedditVideo(url):\n",
    "  # delete video.mp4 if it exists\n",
    "  subprocess.call(['rm', '-f', video_input_file])\n",
    "  reddit = Downloader(max_q=True)\n",
    "  reddit.log = False\n",
    "  reddit.url = url\n",
    "  reddit.path = \"./\"\n",
    "  reddit.filename = video_input_file\n",
    "  reddit.download()\n",
    "  print(\"Downloaded \" + video_input_file)\n",
    "\n",
    "  image = extractFrameFromVideo()\n",
    "\n",
    "  image = Image.open(video_output_file)\n",
    "  return image\n",
    "\n",
    "def extractFrameFromVideo():\n",
    "  input_file = video_input_file\n",
    "  output_file = video_output_file\n",
    "\n",
    "  #delete video.jpg if it exists\n",
    "  subprocess.call(['rm', '-f', output_file])\n",
    "\n",
    "  duration = float(subprocess.check_output(['ffprobe', '-i', input_file, '-show_entries', 'format=duration', '-v', 'quiet', '-of', 'csv=%s' % (\"p=0\")]).strip())\n",
    "  middle_time = duration / 2\n",
    "  #extract the frame\n",
    "  subprocess.call(['ffmpeg', '-i', input_file, '-ss', str(middle_time), '-vframes', '1', '-q:v', '2', output_file], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "  print(\"Extracted frame from \" + input_file + \" to \" + output_file)\n",
    "\n",
    "  image = Image.open(output_file)\n",
    "  return image\n",
    "\n",
    "\n",
    "def generate_blip2(image, context=None):\n",
    "  if context:\n",
    "    inputs = processor(images=image, text=context, return_tensors=\"pt\").to(device, torch.float16)\n",
    "  else:\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "  generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "  generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "  return generated_text\n",
    "\n",
    "def downloadVideo(url):\n",
    "  if 'reddit' in url:\n",
    "    return downloadRedditVideo(url)\n",
    "  else:\n",
    "    video_data = requests.get(url).content\n",
    "    with open(video_input_file, 'wb') as handler:\n",
    "      handler.write(video_data)\n",
    "    \n",
    "    return extractFrameFromVideo()\n",
    "\n",
    "\n",
    "def downloadImage(url):\n",
    "  print(\"Downloading \" + url)\n",
    "  if 'gifv' in url:\n",
    "    # change to gif\n",
    "    url = url.replace('gifv', 'mp4')\n",
    "    return downloadVideo(url)\n",
    "  img_data = requests.get(url).content\n",
    "  with open(img_output_file, 'wb') as handler:\n",
    "      handler.write(img_data)\n",
    "\n",
    "  image = Image.open(img_output_file)\n",
    "  return image\n",
    "\n",
    "\n",
    "def downloadYoutubeThumbnail(url):\n",
    "  if 'youtu.be' in url:\n",
    "    # after / and before ?\n",
    "    youtube_id = url.split('/')[-1]\n",
    "    youtube_id = youtube_id.split('?')[0]\n",
    "    youtube_id = youtube_id.split('&')[0]\n",
    "  else:\n",
    "    youtube_id = url.split('v=')[-1]\n",
    "    youtube_id = youtube_id.split('?')[0]\n",
    "    youtube_id = youtube_id.split('&')[0]\n",
    "  print(\"youtube_id\", youtube_id)\n",
    "  thumbnail_url = 'https://img.youtube.com/vi/' + youtube_id + '/maxresdefault.jpg'\n",
    "  print(\"thumbnail_url\", thumbnail_url)\n",
    "  return downloadImage(thumbnail_url)\n",
    "\n",
    "def downloadRedditMedia(domain, url):\n",
    "  # delete image.jpg if it exists\n",
    "  subprocess.call(['rm', '-f', img_output_file])\n",
    "  #delete video.mp4 if it exists\n",
    "  subprocess.call(['rm', '-f', video_input_file])\n",
    "  if domain in mediaDomains:\n",
    "    return mediaDomains[domain](url)\n",
    "  else:\n",
    "    return None\n",
    "  \n",
    "  # call the right function based on domain\n",
    "  func = mediaDomains[domain]\n",
    "  return func(url)\n",
    "\n",
    "def classifyMedia(url):\n",
    "  for mediaDomain in mediaDomains.keys():\n",
    "    if mediaDomain in url or url.startswith('self.'):\n",
    "      # check if mediaDomain value is downloadImage\n",
    "      if mediaDomains[mediaDomain] == downloadImage:\n",
    "        return \"image\"\n",
    "      else:\n",
    "        return \"video\"\n",
    "  return \"article\"\n",
    "\n",
    "mediaDomains = {\n",
    "    \"i.redd.it\": downloadImage,\n",
    "    \"i.reddituploads.com\": downloadImage,\n",
    "    \"v.redd.it\": downloadRedditVideo,\n",
    "    \"i.imgur.com\": downloadImage,\n",
    "    \"youtu.be\": downloadYoutubeThumbnail,\n",
    "    \"youtube.com\": downloadYoutubeThumbnail,\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image.jpg\n",
    "image = Image.open('image.jpg')\n",
    "generate_blip2(image, \"Question: What is the title of this picture? Answer: german riot police defeated and humiliated by some kind of mud wizard. Question: What is happening? Answer: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "json_files = glob.glob('./data/*.json')\n",
    "print(json_files)\n",
    "\n",
    "import json\n",
    "\n",
    "def getRedditSubreddits(json_files):\n",
    "  subreddits = {}\n",
    "  for json_file in json_files:\n",
    "    with open(json_file) as f:\n",
    "      data = json.load(f)\n",
    "      firstKey = next(iter(data))\n",
    "      subredditName = data[firstKey]['subreddit']\n",
    "      subreddits[subredditName] = data\n",
    "  return subreddits\n",
    "\n",
    "def saveredditSubreddits(subreddits):\n",
    "  for subreddit in subreddits:\n",
    "    with open('./data/' + subreddit + '_top_posts.json', 'w') as outfile:\n",
    "      json.dump(subreddits[subreddit], outfile, indent=4)\n",
    "\n",
    "subreddits = getRedditSubreddits(json_files)\n",
    "\n",
    "print(\"subreddits\", subreddits.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images and caption them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subreddit_name in subreddits:\n",
    "  subreddit = subreddits[subreddit_name]\n",
    "  print(\"subreddit\", subreddit_name)\n",
    "  for post_id in subreddit:\n",
    "    post = subreddit[post_id]\n",
    "    isValidUrlPost = post['url'] != None and post['text'] == None and isMediaDomain(post['url'])\n",
    "    if not isValidUrlPost:\n",
    "      continue\n",
    "\n",
    "    #print url\n",
    "    print(post['url'])\n",
    "    print(post['title'])\n",
    "    image = downloadRedditMedia(post['domain'], post['url'])\n",
    "    #display image\n",
    "    #image.show()\n",
    "    #generate caption\n",
    "    caption = generate_blip2(image)#, \"Question: What is the title of this picture? Answer: \" + post['title'] + \" Question: What is happening? Answer: \")\n",
    "    print(caption)\n",
    "    #save caption\n",
    "    post['text'] = caption\n",
    "    \n",
    "\n",
    "saveredditSubreddits(subreddits)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify each post with image video or text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a ['classification'] field to each post\n",
    "# ['classification'] = 'article' or 'image' or 'video' or 'text'\n",
    "\n",
    "for subreddit_name in subreddits:\n",
    "  subreddit = subreddits[subreddit_name]\n",
    "  print(\"subreddit\", subreddit_name)\n",
    "  for post_id in subreddit:\n",
    "    post = subreddit[post_id]\n",
    "    if post['url'] == None:\n",
    "      continue\n",
    "    if post['is_self'] == True:\n",
    "      post['classification'] = 'text'\n",
    "    else: # cant trust is_video\n",
    "      post['classification'] = classifyMedia(post['url'])\n",
    "    print(post['classification'], post['url'], post['text'])\n",
    "\n",
    "saveredditSubreddits(subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl = []\n",
    "for subreddit_name in subreddits:\n",
    "  subreddit = subreddits[subreddit_name]\n",
    "  #print(\"subreddit\", subreddit_name)\n",
    "  for post_id in subreddit:\n",
    "    post = subreddit[post_id]\n",
    "    _instruction = f\"\"\"You are a Reddit post generator.\"\"\"\n",
    "    _input = f\"\"\"\\nSubreddit: /r/{subreddit_name} \\nAuthor: {post['author_name']} \\nMedia: {post['classification']} \\nTitle: {post['title']} \\nWrite the Reddit post.\"\"\"\n",
    "    _output = post['text']\n",
    "    if _output and not _output.startswith('\\n'): # add newline if not already there, sometimes it is\n",
    "      _output = '\\n' + _output\n",
    "    jsonl_template = {\n",
    "        #\"instruction\": _instruction,\n",
    "        #\"input\": _input,\n",
    "        #\"output\": _output,\n",
    "        \"text\": f\"{_instruction}\\nUser: {_input}\\nAssistant: {_output}\\nUser: How much karma did the post earn?\\nAssistant: {post['score']}\"\n",
    "    }\n",
    "    print(jsonl_template['text'])\n",
    "    jsonl.append(jsonl_template)\n",
    "\n",
    "with open('reddit_posts_vicuna_1.1.jsonl', 'w') as outfile:\n",
    "  for entry in jsonl:\n",
    "    outfile.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingLlama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
