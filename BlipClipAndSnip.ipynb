{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/transformers.git\n",
    "%pip install Pillow redvid\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lettuce/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lettuce/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 5056.42it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:28<00:00, 44.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blip2ForConditionalGeneration(\n",
       "  (vision_model): Blip2VisionModel(\n",
       "    (embeddings): Blip2VisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): Blip2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-38): 39 x Blip2EncoderLayer(\n",
       "          (self_attn): Blip2Attention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Blip2MLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (qformer): Blip2QFormerModel(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (encoder): Blip2QFormerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_projection): Linear(in_features=768, out_features=2560, bias=True)\n",
       "  (language_model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from redvid import Downloader\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def isMediaDomain(url):\n",
    "  for mediaDomain in mediaDomains.keys():\n",
    "    if mediaDomain in url or url.startswith('self.'):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "video_input_file = 'video.mp4'\n",
    "\n",
    "\n",
    "img_output_file = 'image.jpg'\n",
    "video_output_file = img_output_file#'video.jpg'\n",
    "\n",
    "def downloadRedditVideo(url):\n",
    "  # delete video.mp4 if it exists\n",
    "  subprocess.call(['rm', '-f', video_input_file])\n",
    "  reddit = Downloader(max_q=True)\n",
    "  reddit.log = False\n",
    "  reddit.url = url\n",
    "  reddit.path = \"./\"\n",
    "  reddit.filename = video_input_file\n",
    "  reddit.download()\n",
    "  print(\"Downloaded \" + video_input_file)\n",
    "\n",
    "  image = extractFrameFromVideo()\n",
    "\n",
    "  image = Image.open(video_output_file)\n",
    "  return image\n",
    "\n",
    "def extractFrameFromVideo():\n",
    "  input_file = video_input_file\n",
    "  output_file = video_output_file\n",
    "\n",
    "  #delete video.jpg if it exists\n",
    "  subprocess.call(['rm', '-f', output_file])\n",
    "\n",
    "  duration = float(subprocess.check_output(['ffprobe', '-i', input_file, '-show_entries', 'format=duration', '-v', 'quiet', '-of', 'csv=%s' % (\"p=0\")]).strip())\n",
    "  middle_time = duration / 2\n",
    "  #extract the frame\n",
    "  subprocess.call(['ffmpeg', '-i', input_file, '-ss', str(middle_time), '-vframes', '1', '-q:v', '2', output_file], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "  print(\"Extracted frame from \" + input_file + \" to \" + output_file)\n",
    "\n",
    "  image = Image.open(output_file)\n",
    "  return image\n",
    "\n",
    "\n",
    "def generate_blip2(image, context=None):\n",
    "  if context:\n",
    "    inputs = processor(images=image, text=context, return_tensors=\"pt\").to(device, torch.float16)\n",
    "  else:\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "  generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "  generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "  return generated_text\n",
    "\n",
    "def downloadVideo(url):\n",
    "  if 'reddit' in url:\n",
    "    return downloadRedditVideo(url)\n",
    "  else:\n",
    "    video_data = requests.get(url).content\n",
    "    with open(video_input_file, 'wb') as handler:\n",
    "      handler.write(video_data)\n",
    "    \n",
    "    return extractFrameFromVideo()\n",
    "\n",
    "\n",
    "def downloadImage(url):\n",
    "  print(\"Downloading \" + url)\n",
    "  if 'gifv' in url:\n",
    "    # change to gif\n",
    "    url = url.replace('gifv', 'mp4')\n",
    "    return downloadVideo(url)\n",
    "  img_data = requests.get(url).content\n",
    "  with open(img_output_file, 'wb') as handler:\n",
    "      handler.write(img_data)\n",
    "\n",
    "  image = Image.open(img_output_file)\n",
    "  return image\n",
    "\n",
    "\n",
    "def downloadYoutubeThumbnail(url):\n",
    "  if 'youtu.be' in url:\n",
    "    # after / and before ?\n",
    "    youtube_id = url.split('/')[-1]\n",
    "    youtube_id = youtube_id.split('?')[0]\n",
    "    youtube_id = youtube_id.split('&')[0]\n",
    "  else:\n",
    "    youtube_id = url.split('v=')[-1]\n",
    "    youtube_id = youtube_id.split('?')[0]\n",
    "    youtube_id = youtube_id.split('&')[0]\n",
    "  print(\"youtube_id\", youtube_id)\n",
    "  thumbnail_url = 'https://img.youtube.com/vi/' + youtube_id + '/maxresdefault.jpg'\n",
    "  print(\"thumbnail_url\", thumbnail_url)\n",
    "  return downloadImage(thumbnail_url)\n",
    "\n",
    "def downloadRedditMedia(domain, url):\n",
    "  # delete image.jpg if it exists\n",
    "  subprocess.call(['rm', '-f', img_output_file])\n",
    "  #delete video.mp4 if it exists\n",
    "  subprocess.call(['rm', '-f', video_input_file])\n",
    "  if domain in mediaDomains:\n",
    "    return mediaDomains[domain](url)\n",
    "  else:\n",
    "    return None\n",
    "  \n",
    "  # call the right function based on domain\n",
    "  func = mediaDomains[domain]\n",
    "  return func(url)\n",
    "\n",
    "mediaDomains = {\n",
    "    \"i.redd.it\": downloadImage,\n",
    "    \"i.reddituploads.com\": downloadImage,\n",
    "    \"v.redd.it\": downloadRedditVideo,\n",
    "    \"i.imgur.com\": downloadImage,\n",
    "    \"youtu.be\": downloadYoutubeThumbnail,\n",
    "    \"youtube.com\": downloadYoutubeThumbnail,\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~~german~~ german riot police defeated and humiliated by some kind of mud wizard.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image.jpg\n",
    "image = Image.open('image.jpg')\n",
    "generate_blip2(image, \"Question: What is the title of this picture? Answer: german riot police defeated and humiliated by some kind of mud wizard. Question: What is happening? Answer: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/facepalm_top_posts.json', './data/announcements_top_posts.json', './data/DunderMifflin_top_posts.json', './data/wholesomememes_top_posts.json', './data/science_top_posts.json', './data/BikiniBottomTwitter_top_posts.json', './data/freefolk_top_posts.json', './data/MadeMeSmile_top_posts.json', './data/StarWarsBattlefront_top_posts.json', './data/books_top_posts.json', './data/tifu_top_posts.json', './data/nextfuckinglevel_top_posts.json', './data/EarthPorn_top_posts.json', './data/gifs_top_posts.json', './data/AmItheAsshole_top_posts.json', './data/todayilearned_top_posts.json', './data/me_irl_top_posts.json', './data/WatchPeopleDieInside_top_posts.json', './data/thanosdidnothingwrong_top_posts.json', './data/Showerthoughts_top_posts.json', './data/Wellthatsucks_top_posts.json', './data/wallstreetbets_top_posts.json', './data/nottheonion_top_posts.json', './data/comics_top_posts.json', './data/PewdiepieSubmissions_top_posts.json', './data/awfuleverything_top_posts.json', './data/WhitePeopleTwitter_top_posts.json', './data/pics_top_posts.json', './data/IAmA_top_posts.json', './data/MemeEconomy_top_posts.json', './data/LeopardsAteMyFace_top_posts.json', './data/politics_top_posts.json', './data/dankmemes_top_posts.json', './data/videos_top_posts.json', './data/movies_top_posts.json', './data/funny_top_posts.json', './data/aww_top_posts.json', './data/worldnews_top_posts.json', './data/dataisbeautiful_top_posts.json', './data/trashy_top_posts.json', './data/news_top_posts.json', './data/rickandmorty_top_posts.json', './data/assholedesign_top_posts.json', './data/PublicFreakout_top_posts.json', './data/PrequelMemes_top_posts.json', './data/memes_top_posts.json', './data/gaming_top_posts.json', './data/LifeProTips_top_posts.json', './data/interestingasfuck_top_posts.json', './data/space_top_posts.json', './data/mildlyinteresting_top_posts.json', './data/AskReddit_top_posts.json', './data/MurderedByWords_top_posts.json']\n",
      "subreddits dict_keys(['facepalm', 'announcements', 'DunderMifflin', 'wholesomememes', 'science', 'BikiniBottomTwitter', 'freefolk', 'MadeMeSmile', 'StarWarsBattlefront', 'books', 'tifu', 'nextfuckinglevel', 'EarthPorn', 'gifs', 'AmItheAsshole', 'todayilearned', 'me_irl', 'WatchPeopleDieInside', 'thanosdidnothingwrong', 'Showerthoughts', 'Wellthatsucks', 'wallstreetbets', 'nottheonion', 'comics', 'PewdiepieSubmissions', 'awfuleverything', 'WhitePeopleTwitter', 'pics', 'IAmA', 'MemeEconomy', 'LeopardsAteMyFace', 'politics', 'dankmemes', 'videos', 'movies', 'funny', 'aww', 'worldnews', 'dataisbeautiful', 'trashy', 'news', 'rickandmorty', 'assholedesign', 'PublicFreakout', 'PrequelMemes', 'memes', 'gaming', 'LifeProTips', 'interestingasfuck', 'space', 'mildlyinteresting', 'AskReddit', 'MurderedByWords'])\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "json_files = glob.glob('./data/*.json')\n",
    "print(json_files)\n",
    "\n",
    "import json\n",
    "\n",
    "def getRedditSubreddits(json_files):\n",
    "  subreddits = {}\n",
    "  for json_file in json_files:\n",
    "    with open(json_file) as f:\n",
    "      data = json.load(f)\n",
    "      firstKey = next(iter(data))\n",
    "      subredditName = data[firstKey]['subreddit']\n",
    "      subreddits[subredditName] = data\n",
    "  return subreddits\n",
    "\n",
    "def saveredditSubreddits(subreddits):\n",
    "  for subreddit in subreddits:\n",
    "    with open('./data/' + subreddit + '_top_posts.json', 'w') as outfile:\n",
    "      json.dump(subreddits[subreddit], outfile, indent=4)\n",
    "\n",
    "subreddits = getRedditSubreddits(json_files)\n",
    "\n",
    "print(\"subreddits\", subreddits.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit facepalm\n",
      "https://i.redd.it/ypuj1e6u52f81.jpg\n",
      "Math is a Myth.\n",
      "Downloading https://i.redd.it/ypuj1e6u52f81.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a twee with a woman and a man on it\n",
      "https://i.redd.it/2hb9rutko0n61.jpg\n",
      "The state of the world.\n",
      "Downloading https://i.redd.it/2hb9rutko0n61.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my father in law believes a youtube video that two doctors told him vaccines change your dna how is your weekend going?\n",
      "https://i.redd.it/omyb4g6wwby41.jpg\n",
      "Scientific name = poison\n",
      "Downloading https://i.redd.it/omyb4g6wwby41.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a screenshot of facebook messages showing different people\n",
      "https://i.redd.it/7rks4qr3asg51.png\n",
      "the USPS is not a business\n",
      "Downloading https://i.redd.it/7rks4qr3asg51.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the economist is viewed favourably by 99% of americans despite the fact that it loses money every year\n",
      "https://v.redd.it/80n3m81s4fza1\n",
      "YouTuber is facing 20 years in prison after deliberately crashing a plane for views.\n",
      "Downloaded video.mp4\n",
      "Extracted frame from video.mp4 to image.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a small airplane flying over a mountainous area\n",
      "subreddit announcements\n",
      "subreddit DunderMifflin\n",
      "https://v.redd.it/fx3ytdp7vua31\n",
      "I was without internet for a while. Decided to use my talents to make Deangelo actually juggle balls.\n",
      "Downloaded video.mp4\n",
      "Extracted frame from video.mp4 to image.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the office - jim and pam's office\n",
      "https://i.redd.it/4cf58zjff0r41.jpg\n",
      "My take on how characters in the Office refers to the pandemic ( OP: u/DrJesseWelsh, added a few panels)\n",
      "Downloading https://i.redd.it/4cf58zjff0r41.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the office is a show about a group of people in suits\n",
      "https://v.redd.it/hna9mikfm9ta1\n",
      "Rainn Wilson’s Instagram story\n",
      "Downloaded video.mp4\n",
      "Extracted frame from video.mp4 to image.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man sitting on an airplane with a tv on\n",
      "https://i.redd.it/2ss1r3ob3q431.gif\n",
      "Oscar was right....\n",
      "Downloading https://i.redd.it/2ss1r3ob3q431.gif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man and a woman in a room\n",
      "https://i.redd.it/huz74vxpb7n61.jpg\n",
      "I love Angela\n",
      "Downloading https://i.redd.it/huz74vxpb7n61.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angela kirshner, the woman who tweeted about her ex-boyfriend, pamela adams, is now being called a bitch\n",
      "https://i.redd.it/cy74dgiid2u41.jpg\n",
      "Recreation of Asian Jim. My actual wife and kids.\n",
      "Downloading https://i.redd.it/cy74dgiid2u41.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two pictures of a family with a child and a baby\n",
      "https://i.redd.it/ijt1f3ditjk21.jpg\n",
      "Should I call you Jimothy?\n",
      "Downloading https://i.redd.it/ijt1f3ditjk21.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/lettuce/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you take a character from the office and the michael jackson, you're gonna get a jimmy jimmy\n",
      "https://i.imgur.com/KXLKw9W.jpg\n",
      "Custom background for your Zoom and Teams calls. You’re welcome.\n",
      "Downloading https://i.imgur.com/KXLKw9W.jpg\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "conversion not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Woo/reddit_scrape/BlipClipAndSnip.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Woo/reddit_scrape/BlipClipAndSnip.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m image \u001b[39m=\u001b[39m downloadRedditMedia(post[\u001b[39m'\u001b[39m\u001b[39mdomain\u001b[39m\u001b[39m'\u001b[39m], post[\u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Woo/reddit_scrape/BlipClipAndSnip.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#display image\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/Woo/reddit_scrape/BlipClipAndSnip.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m image\u001b[39m.\u001b[39;49mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Woo/reddit_scrape/BlipClipAndSnip.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#generate caption\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Woo/reddit_scrape/BlipClipAndSnip.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m caption \u001b[39m=\u001b[39m generate_blip2(image)\u001b[39m#, \"Question: What is the title of this picture? Answer: \" + post['title'] + \" Question: What is happening? Answer: \")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/Image.py:2492\u001b[0m, in \u001b[0;36mImage.show\u001b[0;34m(self, title)\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(\u001b[39mself\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2473\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2474\u001b[0m \u001b[39m    Displays this image. This method is mainly intended for debugging purposes.\u001b[39;00m\n\u001b[1;32m   2475\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2489\u001b[0m \u001b[39m    :param title: Optional title to use for the image window, where possible.\u001b[39;00m\n\u001b[1;32m   2490\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2492\u001b[0m     _show(\u001b[39mself\u001b[39;49m, title\u001b[39m=\u001b[39;49mtitle)\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/Image.py:3535\u001b[0m, in \u001b[0;36m_show\u001b[0;34m(image, **options)\u001b[0m\n\u001b[1;32m   3532\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_show\u001b[39m(image, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions):\n\u001b[1;32m   3533\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageShow\n\u001b[0;32m-> 3535\u001b[0m     ImageShow\u001b[39m.\u001b[39;49mshow(image, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/ImageShow.py:60\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(image, title, **options)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mDisplay a given image.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m:returns: ``True`` if a suitable viewer was found, ``False`` otherwise.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m viewer \u001b[39min\u001b[39;00m _viewers:\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mif\u001b[39;00m viewer\u001b[39m.\u001b[39;49mshow(image, title\u001b[39m=\u001b[39;49mtitle, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions):\n\u001b[1;32m     61\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/ImageShow.py:84\u001b[0m, in \u001b[0;36mViewer.show\u001b[0;34m(self, image, **options)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[39mif\u001b[39;00m image\u001b[39m.\u001b[39mmode \u001b[39m!=\u001b[39m base:\n\u001b[1;32m     82\u001b[0m         image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mconvert(base)\n\u001b[0;32m---> 84\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshow_image(image, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/ImageShow.py:110\u001b[0m, in \u001b[0;36mViewer.show_image\u001b[0;34m(self, image, **options)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_image\u001b[39m(\u001b[39mself\u001b[39m, image, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions):\n\u001b[1;32m    109\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display the given image.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshow_file(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_image(image), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/ImageShow.py:106\u001b[0m, in \u001b[0;36mViewer.save_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_image\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[1;32m    105\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Save to temporary file and return filename.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39;49m_dump(\u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_format(image), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/Image.py:601\u001b[0m, in \u001b[0;36mImage._dump\u001b[0;34m(self, file, format, **options)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mim\u001b[39m.\u001b[39msave_ppm(filename)\n\u001b[1;32m    600\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 601\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(filename, \u001b[39mformat\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    603\u001b[0m \u001b[39mreturn\u001b[39;00m filename\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/Image.py:2438\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2435\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2438\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[1;32m   2439\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   2440\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/PngImagePlugin.py:1218\u001b[0m, in \u001b[0;36m_save_all\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_save_all\u001b[39m(im, fp, filename):\n\u001b[0;32m-> 1218\u001b[0m     _save(im, fp, filename, save_all\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/PngImagePlugin.py:1392\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1389\u001b[0m     chunk(fp, \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meXIf\u001b[39m\u001b[39m\"\u001b[39m, exif)\n\u001b[1;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m save_all:\n\u001b[0;32m-> 1392\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[1;32m   1393\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     ImageFile\u001b[39m.\u001b[39m_save(im, _idat(fp, chunk), [(\u001b[39m\"\u001b[39m\u001b[39mzip\u001b[39m\u001b[39m\"\u001b[39m, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m im\u001b[39m.\u001b[39msize, \u001b[39m0\u001b[39m, rawmode)])\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/PngImagePlugin.py:1108\u001b[0m, in \u001b[0;36m_write_multiple_frames\u001b[0;34m(im, fp, chunk, rawmode, default_image, append_images)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     im_frame \u001b[39m=\u001b[39m im_frame\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m   1107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1108\u001b[0m     im_frame \u001b[39m=\u001b[39m im_frame\u001b[39m.\u001b[39;49mconvert(rawmode)\n\u001b[1;32m   1109\u001b[0m encoderinfo \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mencoderinfo\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m   1110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(duration, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[0;32m~/miniconda3/envs/trainingLlama2/lib/python3.10/site-packages/PIL/Image.py:1062\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1059\u001b[0m     dither \u001b[39m=\u001b[39m Dither\u001b[39m.\u001b[39mFLOYDSTEINBERG\n\u001b[1;32m   1061\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1062\u001b[0m     im \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mconvert(mode, dither)\n\u001b[1;32m   1063\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         \u001b[39m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: conversion not supported"
     ]
    }
   ],
   "source": [
    "for subreddit_name in subreddits:\n",
    "  subreddit = subreddits[subreddit_name]\n",
    "  print(\"subreddit\", subreddit_name)\n",
    "  for post_id in subreddit:\n",
    "    post = subreddit[post_id]\n",
    "    isValidUrlPost = post['url'] != None and post['text'] == None and isMediaDomain(post['url'])\n",
    "    if not isValidUrlPost:\n",
    "      continue\n",
    "\n",
    "    #print url\n",
    "    print(post['url'])\n",
    "    print(post['title'])\n",
    "    image = downloadRedditMedia(post['domain'], post['url'])\n",
    "    #display image\n",
    "    #image.show()\n",
    "    #generate caption\n",
    "    caption = generate_blip2(image)#, \"Question: What is the title of this picture? Answer: \" + post['title'] + \" Question: What is happening? Answer: \")\n",
    "    print(caption)\n",
    "    #save caption\n",
    "    post['text'] = caption\n",
    "    \n",
    "\n",
    "#saveredditSubreddits(subreddits)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingLlama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
